{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giqpVs8xaBcB"
   },
   "source": [
    "# COSC440 Exercise 01: Introduction to TensorFlow #\n",
    "\n",
    "In this exercise, we will be introducing [TensorFlow](http://tensorflow.org/), a cutting-edge library for developing, and evaluating deep neural network models and a high-level framework, Keras, that has some pre-built TensorFlow components. For this class, **assume all labs and projects will utilize TensorFlow Version 2.X.0**. **[Pytorch](https://pytorch.org/) is an alternative deep learning framework to TensorFlow that will not be supported in this class**.\n",
    "\n",
    "We will walk through how TensorFlow works and some of the basic functionality.\n",
    "\n",
    "**Make sure to run each section and discuss in your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKtgyYQEaBcC"
   },
   "source": [
    "### Import Tensorflow ###\n",
    "\n",
    "Run the following python code, and check in with your group if there are errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B0vzZefVaBcD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f6a43c73-4eea-47d7-882c-3049fdeea4cc",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:02:40.259320Z",
     "start_time": "2024-08-08T05:02:40.252494Z"
    }
   },
   "source": [
    "# Install TensorFlow\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQP2CdcGlfoH"
   },
   "source": [
    "## TensorFlow 101 - The Basics ##\n",
    "\n",
    "Writing code in TensorFlow 2 is similar to writing code in Python. With eager execution (enabled in TensorFlow 2 by default), operations are evaluated immediately in an imperative programming environment. This makes operations return concrete values and makes things easier to debug, essentially what python is doing.\n",
    "\n",
    "###Tensors###\n",
    "Tensors in TensorFlow are objects that represent vectors or matrices. They are effectively represented as a n-dimensional array and they have two properties: a shape and a data type (float32, int32, or string, for example).\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y-gzMX9SmcF1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "outputId": "113eff51-4fab-4727-8583-e676db9b4828",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:02:45.238054Z",
     "start_time": "2024-08-08T05:02:45.229055Z"
    }
   },
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"x = {}\".format(x))\n",
    "print(\"m = {}\".format(m))\n",
    "print(\"hello, {}\".format(m))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [[2.0]]\n",
      "m = [[4.]]\n",
      "hello, [[4.]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPdfc5HNnNS3"
   },
   "source": [
    "Here, m is a Tensor. We can easily inspect the result of the multiplication with a print() statement because tensors are treated as concrete values in Tensorflow 2. Doing so doesn't interfere with training a network either!\n",
    "\n",
    "As for actually setting up a model, we need to introduce a few important tensorflow constructs:\n",
    "\n",
    "###Variables:###\n",
    "\n",
    "Variables are a convenient way to represent trainable weights in your model that are constantly updated. Their values are shared throughout program execution, but can be changed by running operations on it. When using a variable, it must be initialized with starting values. The following are all valid ways of doing this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NSfEACuzwZ0a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "032b176a-332c-4365-bb12-57be13825a0e",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:02:52.818260Z",
     "start_time": "2024-08-08T05:02:52.712292Z"
    }
   },
   "source": [
    "# Using a python list\n",
    "my_variable = tf.Variable([[1.,0.]])\n",
    "print(my_variable)\n",
    "\n",
    "# Initializing variables with a NumPy array\n",
    "my_variable_from_np_array = tf.Variable(np.zeros((3,3)))\n",
    "print(my_variable_from_np_array)\n",
    "\n",
    "# You can also use some tensorflow built in variables\n",
    "gaussian_initialization = tf.Variable(tf.random.normal(shape=[3,3], stddev=.1))\n",
    "print(gaussian_initialization)\n",
    "\n",
    "# To convert a variable from a tensor to a NumPy array, use the numpy() function\n",
    "my_np_variable = tf.Variable([[1., 2., 5.]]).numpy()\n",
    "print(my_np_variable)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[1., 0.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float64, numpy=\n",
      "array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]])>\n",
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[-0.13662481,  0.08411361,  0.04706925],\n",
      "       [ 0.10552702,  0.06126437, -0.09235079],\n",
      "       [ 0.12096814,  0.06565663,  0.05438525]], dtype=float32)>\n",
      "[[1. 2. 5.]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSUfD745wgOK"
   },
   "source": [
    "By default, TensorFlow variables are trainable. This means they will be watched and accounted for during automatic differentiation and backpropagation. For more information on tensorflow variables, check out the documentation:\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Variable?hl=en\n",
    "\n",
    "\n",
    "###Gradient Tape:###\n",
    "\n",
    "To compute the gradients with respect to the loss in our model, we need to make use of tf.GradientTape(). This enables us to record the graph for use with reverse auto differentiation. Any tensor computations inside the tape are accounted for and used during differentiation/backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uJovd7wjyLaY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "91914123-84e5-4e56-d4c8-74e3692b081e",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:04:19.043968Z",
     "start_time": "2024-08-08T05:04:19.010977Z"
    }
   },
   "source": [
    "w = tf.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = w + w\n",
    "\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBxu_HuizQk9"
   },
   "source": [
    "Here the loss is calculated as `w + w` within the scope of `tf.GradientTape()`, so we are able to differentiate the loss with respect to the variable, `w`. This gradient is calculated in the call to `tape.gradient()`. GradientTape() can also compute gradients for non-trainable variables/tensors by using the `watch()` function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i52hWLpg2z5N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4035d9a7-8632-40c8-f9e7-5a3196deb6f8",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:04:46.461621Z",
     "start_time": "2024-08-08T05:04:46.449624Z"
    }
   },
   "source": [
    "x = tf.ones((1, 1)) #Not a trainable variable!\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  loss = x + x\n",
    "\n",
    "grad = tape.gradient(loss, x)\n",
    "print(grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cTgoGmUWgP8"
   },
   "source": [
    "For tips and tricks on automatic differentiation, check out the documentation: https://www.tensorflow.org/tutorials/eager/automatic_differentiation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmrP-1kOXOgT"
   },
   "source": [
    "### Activation functions: Relu ###\n",
    "\n",
    "Here is an example of calling the activation function Relu with our tensorflow variable:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dGHismBqXQO5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cfae9c8e-9ed7-48b1-de9f-c86f0ceaa7a9",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:05:15.299569Z",
     "start_time": "2024-08-08T05:05:15.284784Z"
    }
   },
   "source": [
    "gaussian_initialization = tf.Variable(tf.random.normal(shape=[3,3], stddev=.1))\n",
    "print(gaussian_initialization)\n",
    "\n",
    "print(tf.nn.relu(gaussian_initialization))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[-0.06526665,  0.21471539,  0.06197277],\n",
      "       [ 0.07615405, -0.05479568, -0.02306583],\n",
      "       [ 0.02092689,  0.03617748,  0.0339775 ]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[0.         0.21471539 0.06197277]\n",
      " [0.07615405 0.         0.        ]\n",
      " [0.02092689 0.03617748 0.0339775 ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBtc66ayAJ0T"
   },
   "source": [
    "### Optimizers:###\n",
    "Once we have the gradient computed with the GradientTape, we must update our weights for our model to train. For this we need to use an optimizer. In this class, you can use the Adam Optimizer. This optimizer is pretty standard, but examples of different types of optimizers are as follows:\n",
    "\n",
    "\n",
    "```\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) #Stochastic Gradient Descent\n",
    "adagrad_optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "rms_prop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG12L6wIWr6v"
   },
   "source": [
    "For now, don't worry about the details of how they work. You'll see soon in a future exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjilAbLe3KKp"
   },
   "source": [
    "\n",
    "\n",
    "## Putting it all Together##\n",
    "Now let's use everything we've learned to create a TensorFlow 2 neural network. Note that because you are still working on Assignment 1, I have directly created a one layer dense neural network using the Keras package implementation. Some of the early exercises and assignments in this course require you to construct each underlying method for your Model instead of using built in components.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ENHI1VJk9EhC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6a9ee660-6d33-4f4f-e92a-da9988b2163f",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:06:13.723805Z",
     "start_time": "2024-08-08T05:06:13.702246Z"
    }
   },
   "source": [
    "# Instantiate our model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation = 'relu', input_shape = (784,)))"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-sjZj719i2e"
   },
   "source": [
    "###tf.one_hot():###\n",
    "\n",
    "Next, we load in our data as well as initialize our model and optimizer. We'd like to turn our labels into one hot vectors. A one hot encoding is a vector representation where all the elements of the vector are 0 except one, which has 1 as its value. For example, [0 0 0 1 0 0] is a one-hot vector. Check out [this](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/one_hot)  documentation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rZhy5n-Rb5JK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d87696ec-9f09-44ad-897b-25363b712829",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:06:40.243909Z",
     "start_time": "2024-08-08T05:06:36.710184Z"
    }
   },
   "source": [
    "# Loading in and preprocessing the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "# x_train is your train inputs\n",
    "# y_train is your train labels\n",
    "# x_test is your test inputs\n",
    "# y_test is your test labels\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0 # normalizing data\n",
    "x_train = x_train.astype(np.float32)\n",
    "\n",
    "# Make labels one hot vectors\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_test = tf.one_hot(y_test, depth=10)\n",
    "\n",
    "# Choosing our loss function, optimizer, and metric\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "metric = tf.keras.metrics.Accuracy()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001B[1m11490434/11490434\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 0us/step\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8lW85kjcEuL"
   },
   "source": [
    "Finally, we train using examples from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DM_Fqz95v8Ww",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8dc76224-426c-4640-dee9-d6181a93d7e9",
    "ExecuteTime": {
     "end_time": "2024-08-08T05:10:46.382521Z",
     "start_time": "2024-08-08T05:06:57.896455Z"
    }
   },
   "source": [
    "# Loop through 10000 training images\n",
    "for i in range(10000):\n",
    "  image = np.reshape(x_train[i], (1,-1))\n",
    "  label = np.reshape(y_train[i], (1,-1))\n",
    "\n",
    "  # Implement backprop:\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(image) # this calls the call function conveniently\n",
    "    loss = loss_function(predictions, label)\n",
    "\n",
    "    if i % 500 == 0:\n",
    "      metric.update_state(model(x_train.reshape(-1,784)), y_train)\n",
    "      train_acc = metric.result().numpy()\n",
    "      print(\"Accuracy on training set after {} training steps: {}\".format(i, train_acc))\n",
    "\n",
    "  # The keras Model class has the computed property trainable_variables to conveniently\n",
    "  # return all the trainable variables you'd want to adjust based on the gradients\n",
    "\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set after 0 training steps: 0.5202049016952515\n",
      "Accuracy on training set after 500 training steps: 0.686976432800293\n",
      "Accuracy on training set after 1000 training steps: 0.7475413084030151\n",
      "Accuracy on training set after 1500 training steps: 0.7752020955085754\n",
      "Accuracy on training set after 2000 training steps: 0.7935642004013062\n",
      "Accuracy on training set after 2500 training steps: 0.8024643063545227\n",
      "Accuracy on training set after 3000 training steps: 0.8101615309715271\n",
      "Accuracy on training set after 3500 training steps: 0.8156802654266357\n",
      "Accuracy on training set after 4000 training steps: 0.8215883374214172\n",
      "Accuracy on training set after 4500 training steps: 0.8258597254753113\n",
      "Accuracy on training set after 5000 training steps: 0.8289491534233093\n",
      "Accuracy on training set after 5500 training steps: 0.8315682411193848\n",
      "Accuracy on training set after 6000 training steps: 0.8337570428848267\n",
      "Accuracy on training set after 6500 training steps: 0.8355081677436829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 20\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# The keras Model class has the computed property trainable_variables to conveniently\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# return all the trainable variables you'd want to adjust based on the gradients\u001B[39;00m\n\u001B[0;32m     19\u001B[0m gradients \u001B[38;5;241m=\u001B[39m tape\u001B[38;5;241m.\u001B[39mgradient(loss, model\u001B[38;5;241m.\u001B[39mtrainable_variables)\n\u001B[1;32m---> 20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mapply_gradients(\u001B[38;5;28mzip\u001B[39m(gradients, model\u001B[38;5;241m.\u001B[39mtrainable_variables))\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:282\u001B[0m, in \u001B[0;36mBaseOptimizer.apply_gradients\u001B[1;34m(self, grads_and_vars)\u001B[0m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_gradients\u001B[39m(\u001B[38;5;28mself\u001B[39m, grads_and_vars):\n\u001B[0;32m    281\u001B[0m     grads, trainable_variables \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mgrads_and_vars)\n\u001B[1;32m--> 282\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply(grads, trainable_variables)\n\u001B[0;32m    283\u001B[0m     \u001B[38;5;66;03m# Return iterations for compat with tf.keras.\u001B[39;00m\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterations\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:351\u001B[0m, in \u001B[0;36mBaseOptimizer.apply\u001B[1;34m(self, grads, trainable_variables)\u001B[0m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_weight_decay(trainable_variables)\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# Apply gradient updates.\u001B[39;00m\n\u001B[1;32m--> 351\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend_apply_gradients(grads, trainable_variables)\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# Apply variable constraints after applying gradients.\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m variable \u001B[38;5;129;01min\u001B[39;00m trainable_variables:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:405\u001B[0m, in \u001B[0;36mBaseOptimizer._backend_apply_gradients\u001B[1;34m(self, grads, trainable_variables)\u001B[0m\n\u001B[0;32m    396\u001B[0m     ops\u001B[38;5;241m.\u001B[39mcond(\n\u001B[0;32m    397\u001B[0m         is_update_step,\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m: _update_step_fn(grads, trainable_variables),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    401\u001B[0m         ),\n\u001B[0;32m    402\u001B[0m     )\n\u001B[0;32m    403\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    404\u001B[0m     \u001B[38;5;66;03m# Run udpate step.\u001B[39;00m\n\u001B[1;32m--> 405\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend_update_step(\n\u001B[0;32m    406\u001B[0m         grads, trainable_variables, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate\n\u001B[0;32m    407\u001B[0m     )\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_ema:\n\u001B[0;32m    410\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_model_variables_moving_average(\n\u001B[0;32m    411\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_trainable_variables\n\u001B[0;32m    412\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:119\u001B[0m, in \u001B[0;36mTFOptimizer._backend_update_step\u001B[1;34m(self, grads, trainable_variables, learning_rate)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_backend_update_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, grads, trainable_variables, learning_rate):\n\u001B[0;32m    115\u001B[0m     trainable_variables \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    116\u001B[0m         v\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, backend\u001B[38;5;241m.\u001B[39mVariable) \u001B[38;5;28;01melse\u001B[39;00m v\n\u001B[0;32m    117\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m trainable_variables\n\u001B[0;32m    118\u001B[0m     ]\n\u001B[1;32m--> 119\u001B[0m     tf\u001B[38;5;241m.\u001B[39m__internal__\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39minterim\u001B[38;5;241m.\u001B[39mmaybe_merge_call(\n\u001B[0;32m    120\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_distributed_tf_update_step,\n\u001B[0;32m    121\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_distribution_strategy,\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(grads, trainable_variables)),\n\u001B[0;32m    123\u001B[0m         learning_rate,\n\u001B[0;32m    124\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001B[0m, in \u001B[0;36mmaybe_merge_call\u001B[1;34m(fn, strategy, *args, **kwargs)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \n\u001B[0;32m     33\u001B[0m \u001B[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;124;03m  The return value of the `fn` call.\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m strategy_supports_no_merge_call():\n\u001B[1;32m---> 51\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(strategy, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m distribute_lib\u001B[38;5;241m.\u001B[39mget_replica_context()\u001B[38;5;241m.\u001B[39mmerge_call(\n\u001B[0;32m     54\u001B[0m       fn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:135\u001B[0m, in \u001B[0;36mTFOptimizer._distributed_tf_update_step\u001B[1;34m(self, distribution, grads_and_vars, learning_rate)\u001B[0m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_step(grad, var, learning_rate)\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m grad, var \u001B[38;5;129;01min\u001B[39;00m grads_and_vars:\n\u001B[1;32m--> 135\u001B[0m     distribution\u001B[38;5;241m.\u001B[39mextended\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[0;32m    136\u001B[0m         var,\n\u001B[0;32m    137\u001B[0m         apply_grad_to_update_var,\n\u001B[0;32m    138\u001B[0m         args\u001B[38;5;241m=\u001B[39m(grad, learning_rate),\n\u001B[0;32m    139\u001B[0m         group\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    140\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3005\u001B[0m, in \u001B[0;36mStrategyExtendedV2.update\u001B[1;34m(self, var, fn, args, kwargs, group)\u001B[0m\n\u001B[0;32m   3002\u001B[0m   fn \u001B[38;5;241m=\u001B[39m autograph\u001B[38;5;241m.\u001B[39mtf_convert(\n\u001B[0;32m   3003\u001B[0m       fn, autograph_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx(), convert_by_default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   3004\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy()\u001B[38;5;241m.\u001B[39mscope():\n\u001B[1;32m-> 3005\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update(var, fn, args, kwargs, group)\n\u001B[0;32m   3006\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3007\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_replica_ctx_update(\n\u001B[0;32m   3008\u001B[0m       var, fn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs, group\u001B[38;5;241m=\u001B[39mgroup)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4075\u001B[0m, in \u001B[0;36m_DefaultDistributionExtended._update\u001B[1;34m(self, var, fn, args, kwargs, group)\u001B[0m\n\u001B[0;32m   4072\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update\u001B[39m(\u001B[38;5;28mself\u001B[39m, var, fn, args, kwargs, group):\n\u001B[0;32m   4073\u001B[0m   \u001B[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001B[39;00m\n\u001B[0;32m   4074\u001B[0m   \u001B[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001B[39;00m\n\u001B[1;32m-> 4075\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_non_slot(var, fn, (var,) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mtuple\u001B[39m(args), kwargs, group)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4081\u001B[0m, in \u001B[0;36m_DefaultDistributionExtended._update_non_slot\u001B[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001B[0m\n\u001B[0;32m   4077\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update_non_slot\u001B[39m(\u001B[38;5;28mself\u001B[39m, colocate_with, fn, args, kwargs, should_group):\n\u001B[0;32m   4078\u001B[0m   \u001B[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001B[39;00m\n\u001B[0;32m   4079\u001B[0m   \u001B[38;5;66;03m# once that value is used for something.\u001B[39;00m\n\u001B[0;32m   4080\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m UpdateContext(colocate_with):\n\u001B[1;32m-> 4081\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   4082\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m should_group:\n\u001B[0;32m   4083\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:596\u001B[0m, in \u001B[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    595\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ag_ctx\u001B[38;5;241m.\u001B[39mControlStatusCtx(status\u001B[38;5;241m=\u001B[39mag_ctx\u001B[38;5;241m.\u001B[39mStatus\u001B[38;5;241m.\u001B[39mUNSPECIFIED):\n\u001B[1;32m--> 596\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:132\u001B[0m, in \u001B[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001B[1;34m(var, grad, learning_rate)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_grad_to_update_var\u001B[39m(var, grad, learning_rate):\n\u001B[1;32m--> 132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_step(grad, var, learning_rate)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:133\u001B[0m, in \u001B[0;36mAdam.update_step\u001B[1;34m(self, gradient, variable, learning_rate)\u001B[0m\n\u001B[0;32m    128\u001B[0m v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_velocities[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_variable_index(variable)]\n\u001B[0;32m    130\u001B[0m alpha \u001B[38;5;241m=\u001B[39m lr \u001B[38;5;241m*\u001B[39m ops\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta_2_power) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta_1_power)\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massign_add(\n\u001B[1;32m--> 133\u001B[0m     m, ops\u001B[38;5;241m.\u001B[39mmultiply(ops\u001B[38;5;241m.\u001B[39msubtract(gradient, m), \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta_1)\n\u001B[0;32m    134\u001B[0m )\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massign_add(\n\u001B[0;32m    136\u001B[0m     v,\n\u001B[0;32m    137\u001B[0m     ops\u001B[38;5;241m.\u001B[39mmultiply(\n\u001B[0;32m    138\u001B[0m         ops\u001B[38;5;241m.\u001B[39msubtract(ops\u001B[38;5;241m.\u001B[39msquare(gradient), v), \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta_2\n\u001B[0;32m    139\u001B[0m     ),\n\u001B[0;32m    140\u001B[0m )\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mamsgrad:\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\ops\\numpy.py:5437\u001B[0m, in \u001B[0;36msubtract\u001B[1;34m(x1, x2)\u001B[0m\n\u001B[0;32m   5435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m any_symbolic_tensors((x1, x2)):\n\u001B[0;32m   5436\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Subtract()\u001B[38;5;241m.\u001B[39msymbolic_call(x1, x2)\n\u001B[1;32m-> 5437\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m backend\u001B[38;5;241m.\u001B[39mnumpy\u001B[38;5;241m.\u001B[39msubtract(x1, x2)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\sparse.py:493\u001B[0m, in \u001B[0;36melementwise_binary_union.<locals>.wrap_elementwise_binary_union.<locals>.sparse_wrapper\u001B[1;34m(x1, x2)\u001B[0m\n\u001B[0;32m    490\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x2, tf\u001B[38;5;241m.\u001B[39mIndexedSlices):\n\u001B[0;32m    491\u001B[0m     \u001B[38;5;66;03m# x2 is an IndexedSlices, densify.\u001B[39;00m\n\u001B[0;32m    492\u001B[0m     x2 \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x2)\n\u001B[1;32m--> 493\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(x1, x2)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py:346\u001B[0m, in \u001B[0;36msubtract\u001B[1;34m(x1, x2)\u001B[0m\n\u001B[0;32m    344\u001B[0m x1 \u001B[38;5;241m=\u001B[39m convert_to_tensor(x1, dtype)\n\u001B[0;32m    345\u001B[0m x2 \u001B[38;5;241m=\u001B[39m convert_to_tensor(x2, dtype)\n\u001B[1;32m--> 346\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39msubtract(x1, x2)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1258\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[0;32m   1259\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1260\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_target(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1261\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m   1262\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[0;32m   1263\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[0;32m   1264\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:545\u001B[0m, in \u001B[0;36msubtract\u001B[1;34m(x, y, name)\u001B[0m\n\u001B[0;32m    541\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmath.subtract\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubtract\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    542\u001B[0m \u001B[38;5;129m@dispatch\u001B[39m\u001B[38;5;241m.\u001B[39mregister_binary_elementwise_api\n\u001B[0;32m    543\u001B[0m \u001B[38;5;129m@dispatch\u001B[39m\u001B[38;5;241m.\u001B[39madd_dispatch_support\n\u001B[0;32m    544\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msubtract\u001B[39m(x, y, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 545\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m gen_math_ops\u001B[38;5;241m.\u001B[39msub(x, y, name)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001B[0m, in \u001B[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    141\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mis_auto_dtype_conversion_enabled():\n\u001B[1;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    143\u001B[0m   bound_arguments \u001B[38;5;241m=\u001B[39m signature\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    144\u001B[0m   bound_arguments\u001B[38;5;241m.\u001B[39mapply_defaults()\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:13682\u001B[0m, in \u001B[0;36msub\u001B[1;34m(x, y, name)\u001B[0m\n\u001B[0;32m  13680\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m  13681\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m> 13682\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m sub_eager_fallback(\n\u001B[0;32m  13683\u001B[0m       x, y, name\u001B[38;5;241m=\u001B[39mname, ctx\u001B[38;5;241m=\u001B[39m_ctx)\n\u001B[0;32m  13684\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_SymbolicException:\n\u001B[0;32m  13685\u001B[0m   \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:13702\u001B[0m, in \u001B[0;36msub_eager_fallback\u001B[1;34m(x, y, name, ctx)\u001B[0m\n\u001B[0;32m  13701\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msub_eager_fallback\u001B[39m(x: Annotated[Any, TV_Sub_T], y: Annotated[Any, TV_Sub_T], name, ctx) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Annotated[Any, TV_Sub_T]:\n\u001B[1;32m> 13702\u001B[0m   _attr_T, _inputs_T \u001B[38;5;241m=\u001B[39m _execute\u001B[38;5;241m.\u001B[39margs_to_matching_eager([x, y], ctx, [_dtypes\u001B[38;5;241m.\u001B[39mbfloat16, _dtypes\u001B[38;5;241m.\u001B[39mhalf, _dtypes\u001B[38;5;241m.\u001B[39mfloat32, _dtypes\u001B[38;5;241m.\u001B[39mfloat64, _dtypes\u001B[38;5;241m.\u001B[39muint8, _dtypes\u001B[38;5;241m.\u001B[39mint8, _dtypes\u001B[38;5;241m.\u001B[39muint16, _dtypes\u001B[38;5;241m.\u001B[39mint16, _dtypes\u001B[38;5;241m.\u001B[39mint32, _dtypes\u001B[38;5;241m.\u001B[39mint64, _dtypes\u001B[38;5;241m.\u001B[39mcomplex64, _dtypes\u001B[38;5;241m.\u001B[39mcomplex128, _dtypes\u001B[38;5;241m.\u001B[39muint32, _dtypes\u001B[38;5;241m.\u001B[39muint64, ])\n\u001B[0;32m  13703\u001B[0m   (x, y) \u001B[38;5;241m=\u001B[39m _inputs_T\n\u001B[0;32m  13704\u001B[0m   _inputs_flat \u001B[38;5;241m=\u001B[39m [x, y]\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:267\u001B[0m, in \u001B[0;36margs_to_matching_eager\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m    265\u001B[0m       dtype \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 267\u001B[0m   ret \u001B[38;5;241m=\u001B[39m [tensor_conversion_registry\u001B[38;5;241m.\u001B[39mconvert(t, dtype) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m l]\n\u001B[0;32m    269\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001B[39;00m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    271\u001B[0m keras_symbolic_tensors \u001B[38;5;241m=\u001B[39m [x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m ret \u001B[38;5;28;01mif\u001B[39;00m _is_keras_symbolic_tensor(x)]\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:209\u001B[0m, in \u001B[0;36mconvert\u001B[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001B[0m\n\u001B[0;32m    207\u001B[0m overload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(value, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__tf_tensor__\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m overload \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 209\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m overload(dtype, name)  \u001B[38;5;66;03m#  pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m base_type, conversion_func \u001B[38;5;129;01min\u001B[39;00m get(\u001B[38;5;28mtype\u001B[39m(value)):\n\u001B[0;32m    212\u001B[0m   \u001B[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001B[39;00m\n\u001B[0;32m    213\u001B[0m   \u001B[38;5;66;03m# cast to preferred_dtype first.\u001B[39;00m\n\u001B[0;32m    214\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:65\u001B[0m, in \u001B[0;36mVariable.__tf_tensor__\u001B[1;34m(self, dtype, name)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__tf_tensor__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mconvert_to_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue, dtype\u001B[38;5;241m=\u001B[39mdtype, name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001B[0m, in \u001B[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1258\u001B[0m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[0;32m   1259\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1260\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_target(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1261\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m   1262\u001B[0m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[0;32m   1263\u001B[0m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[0;32m   1264\u001B[0m   result \u001B[38;5;241m=\u001B[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:161\u001B[0m, in \u001B[0;36mconvert_to_tensor_v2_with_dispatch\u001B[1;34m(value, dtype, dtype_hint, name)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m\u001B[38;5;241m.\u001B[39mtf_export(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconvert_to_tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[0;32m     97\u001B[0m \u001B[38;5;129m@dispatch\u001B[39m\u001B[38;5;241m.\u001B[39madd_dispatch_support\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_to_tensor_v2_with_dispatch\u001B[39m(\n\u001B[0;32m     99\u001B[0m     value, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype_hint\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    100\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m tensor_lib\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m    101\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001B[39;00m\n\u001B[0;32m    102\u001B[0m \n\u001B[0;32m    103\u001B[0m \u001B[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 161\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_tensor_v2(\n\u001B[0;32m    162\u001B[0m       value, dtype\u001B[38;5;241m=\u001B[39mdtype, dtype_hint\u001B[38;5;241m=\u001B[39mdtype_hint, name\u001B[38;5;241m=\u001B[39mname\n\u001B[0;32m    163\u001B[0m   )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:171\u001B[0m, in \u001B[0;36mconvert_to_tensor_v2\u001B[1;34m(value, dtype, dtype_hint, name)\u001B[0m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001B[39;00m\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tensor_conversion_registry\u001B[38;5;241m.\u001B[39mconvert(\n\u001B[0;32m    172\u001B[0m     value, dtype, name, preferred_dtype\u001B[38;5;241m=\u001B[39mdtype_hint\n\u001B[0;32m    173\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001B[0m, in \u001B[0;36mconvert\u001B[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001B[0m\n\u001B[0;32m    225\u001B[0m       \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    226\u001B[0m           _add_error_prefix(\n\u001B[0;32m    227\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConversion function \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconversion_func\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m for type \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    230\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mactual = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mret\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mbase_dtype\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    231\u001B[0m               name\u001B[38;5;241m=\u001B[39mname))\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 234\u001B[0m   ret \u001B[38;5;241m=\u001B[39m conversion_func(value, dtype\u001B[38;5;241m=\u001B[39mdtype, name\u001B[38;5;241m=\u001B[39mname, as_ref\u001B[38;5;241m=\u001B[39mas_ref)\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ret \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[0;32m    237\u001B[0m   \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:2375\u001B[0m, in \u001B[0;36m_dense_var_to_tensor\u001B[1;34m(var, dtype, name, as_ref)\u001B[0m\n\u001B[0;32m   2374\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_dense_var_to_tensor\u001B[39m(var, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, as_ref\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m-> 2375\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m var\u001B[38;5;241m.\u001B[39m_dense_var_to_tensor(dtype\u001B[38;5;241m=\u001B[39mdtype, name\u001B[38;5;241m=\u001B[39mname, as_ref\u001B[38;5;241m=\u001B[39mas_ref)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1621\u001B[0m, in \u001B[0;36mBaseResourceVariable._dense_var_to_tensor\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m   1619\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_value()\u001B[38;5;241m.\u001B[39mop\u001B[38;5;241m.\u001B[39minputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1620\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1621\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue()\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:656\u001B[0m, in \u001B[0;36mBaseResourceVariable.value\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    654\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_value\n\u001B[0;32m    655\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mcolocate_with(\u001B[38;5;28;01mNone\u001B[39;00m, ignore_existing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m--> 656\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_variable_op()\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:841\u001B[0m, in \u001B[0;36mBaseResourceVariable._read_variable_op\u001B[1;34m(self, no_copy)\u001B[0m\n\u001B[0;32m    839\u001B[0m       result \u001B[38;5;241m=\u001B[39m read_and_set_handle(no_copy)\n\u001B[0;32m    840\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 841\u001B[0m   result \u001B[38;5;241m=\u001B[39m read_and_set_handle(no_copy)\n\u001B[0;32m    843\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m    844\u001B[0m   \u001B[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001B[39;00m\n\u001B[0;32m    845\u001B[0m   \u001B[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001B[39;00m\n\u001B[0;32m    846\u001B[0m   record\u001B[38;5;241m.\u001B[39mrecord_operation(\n\u001B[0;32m    847\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReadVariableOp\u001B[39m\u001B[38;5;124m\"\u001B[39m, [result], [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle],\n\u001B[0;32m    848\u001B[0m       backward_function\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: [x],\n\u001B[0;32m    849\u001B[0m       forward_function\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: [x])\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:831\u001B[0m, in \u001B[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001B[1;34m(no_copy)\u001B[0m\n\u001B[0;32m    829\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m no_copy \u001B[38;5;129;01mand\u001B[39;00m forward_compat\u001B[38;5;241m.\u001B[39mforward_compatible(\u001B[38;5;241m2022\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m3\u001B[39m):\n\u001B[0;32m    830\u001B[0m   gen_resource_variable_ops\u001B[38;5;241m.\u001B[39mdisable_copy_on_read(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle)\n\u001B[1;32m--> 831\u001B[0m result \u001B[38;5;241m=\u001B[39m gen_resource_variable_ops\u001B[38;5;241m.\u001B[39mread_variable_op(\n\u001B[0;32m    832\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dtype)\n\u001B[0;32m    833\u001B[0m _maybe_set_handle_data(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dtype, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle, result)\n\u001B[0;32m    834\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:590\u001B[0m, in \u001B[0;36mread_variable_op\u001B[1;34m(resource, dtype, name)\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[0;32m    589\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 590\u001B[0m     _result \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_FastPathExecute(\n\u001B[0;32m    591\u001B[0m       _ctx, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReadVariableOp\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, resource, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, dtype)\n\u001B[0;32m    592\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m    593\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDDiOD8gKsTN"
   },
   "source": [
    "###Note about model.trainable_variables:###\n",
    "In the last 2 lines of the training loop above, we used `model.trainable_variables` to a get a list of all learnable variables in our model (`W1, b1, W2, b2` in this case). This feature was only available because our model subclasses `tf.Keras.Model`. In some exercises in this course we will optimize gradients and do backprop without subclassing `tf.Keras.Model` by passing in a list or array of the model's trainable variables manually like so:\n",
    "\n",
    "`gradients = tape.gradient(loss, [model.W1, model.b1, model.W2, model.b2]))`\n",
    "\n",
    "`optimizer.apply_gradients(zip(gradients, [model.W1, model.b1, model.W2, model.b2]))`\n",
    "\n",
    "**Training accuracy should be near 87%**\n",
    "\n",
    "**Check in with your group to ensure you have training accuracy at or near 87%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLiV8M15aBcG"
   },
   "source": [
    "## Under the hood: The computation graph##\n",
    "\n",
    "Back in TensorFlow 1, the only way to write code was by creating a computation graph that defines the \"flow\" of Tensors (thus the name TensorFlow) which would be run later. This is the opposite of the eager execution that is now default in TensorFlow 2.0. A computation graph may still be used, but you will not be ever required to use it in this class, since using eager execution is considered standard now. However, it might be helpful for your understanding of deep learning to try to visualize this computation graph:\n",
    "  + This graph consists of nodes and edges. Nodes are **Tensors**, or matrices of varying dimensions (i.e 3D, 4D, etc.). The edges are **Operations** that take one or more tensors, and produce a new, resulting Tensor after applying a given transformation (i.e. addition, subtraction, matrix multiplication, etc.)\n",
    "  + All of these Tensors and Operations exist in this separate, high performing process. That means you can't print/peek into the Tensors like you would in TensorFlow 2.\n",
    "  + The operations in the graph would be done with special variables called **placeholders**. Placeholders would provide a point of input that could be filled in later when the computation is actually formed. This allows for the same graph to be used for many train/test iterations.\n",
    "  + Each of these iterations would be done by calling the run function of a **Session** object with a feed dictionary of all the values of the placeholders to use for this iteration.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1BpmERwt0-dIqVipEu7wozEJLb2fBGHIY\" alt=\"gif.jpg\" width=\"400\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtpAEa9hS9yt"
   },
   "source": [
    "## Acknowledgements & Sources ##\n",
    "\n",
    "This exercise is modified from one written by Tim Ossowski and Rohin Bhushan, with edits by the HTAs and James Okun."
   ]
  }
 ]
}
